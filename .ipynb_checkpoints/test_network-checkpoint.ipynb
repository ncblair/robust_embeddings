{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from models.base_network import BaseNetwork\n",
    "from file_handling.load_datasets import load_mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from preprocessing.noise_models import gaussian_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: 255*x)])\n",
    "test_data = datasets.CIFAR10(root='./cifar-10', train=False, download=False, transform=transform)\n",
    "train_data = datasets.CIFAR10(root='./cifar-10', train=True, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 59.,  43.,  50.,  ..., 158., 152., 148.],\n",
       "         [ 16.,   0.,  18.,  ..., 123., 119., 122.],\n",
       "         [ 25.,  16.,  49.,  ..., 118., 120., 109.],\n",
       "         ...,\n",
       "         [208., 201., 198.,  ..., 160.,  56.,  53.],\n",
       "         [180., 173., 186.,  ..., 184.,  97.,  83.],\n",
       "         [177., 168., 179.,  ..., 216., 151., 123.]],\n",
       "\n",
       "        [[ 62.,  46.,  48.,  ..., 132., 125., 124.],\n",
       "         [ 20.,   0.,   8.,  ...,  88.,  83.,  87.],\n",
       "         [ 24.,   7.,  27.,  ...,  84.,  84.,  73.],\n",
       "         ...,\n",
       "         [170., 153., 161.,  ..., 133.,  31.,  34.],\n",
       "         [139., 123., 144.,  ..., 148.,  62.,  53.],\n",
       "         [144., 129., 142.,  ..., 184., 118.,  92.]],\n",
       "\n",
       "        [[ 63.,  45.,  43.,  ..., 108., 102., 103.],\n",
       "         [ 20.,   0.,   0.,  ...,  55.,  50.,  57.],\n",
       "         [ 21.,   0.,   8.,  ...,  50.,  50.,  42.],\n",
       "         ...,\n",
       "         [ 96.,  34.,  26.,  ...,  70.,   7.,  20.],\n",
       "         [ 96.,  42.,  30.,  ...,  94.,  34.,  34.],\n",
       "         [116.,  94.,  87.,  ..., 140.,  84.,  72.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reg_net = BaseNetwork(\"reg_net\", [(3, 16, 5),(16,32,5),(800,5000), (5000,100),(100,10)],\n",
    "#                                 [\"conv\", \"conv\", \"fc\", \"fc\", \"fc\"])\n",
    "reg_net = BaseNetwork(\"reg_net\", [(3*1024,5000),(5000,100),(100,10)],\n",
    "                                [\"fc\", \"fc\", \"fc\"])\n",
    "reg_net_opt = optim.SGD(reg_net.parameters(), lr=0.00015, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_net = BaseNetwork(\"emb_net\", [(3, 16, 5),(16,32,5), (800,5000),(5000,100),(100,10)],\n",
    "#                                 [\"conv\", \"conv\", \"emb\", \"fc\", \"fc\"])\n",
    "emb_net = BaseNetwork(\"emb_net\", [(3*1024,5000),(5000,100),(100,10)],\n",
    "                                [\"emb\", \"fc\", \"fc\"])\n",
    "emb_net_opt = optim.SGD(emb_net.parameters(), lr=0.05, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c510b84d8549c085d15a7d50daebef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    16] loss: 2.30106\n",
      "[1,    32] loss: 2.31283\n",
      "[1,    48] loss: 2.30354\n",
      "[1,    64] loss: 2.32058\n",
      "[1,    80] loss: 2.30881\n",
      "[1,    96] loss: 2.31354\n",
      "[1,   112] loss: 2.31029\n",
      "[1,   128] loss: 2.30898\n",
      "[1,   144] loss: 2.30271\n",
      "[1,   160] loss: 2.30429\n",
      "[1,   176] loss: 2.29985\n",
      "[1,   192] loss: 2.29576\n",
      "[1,   208] loss: 2.29453\n",
      "[1,   224] loss: 2.28320\n",
      "[1,   240] loss: 2.27474\n",
      "[1,   256] loss: 2.28868\n",
      "[1,   272] loss: 2.25877\n",
      "[1,   288] loss: 2.26135\n",
      "[1,   304] loss: 2.25397\n",
      "[1,   320] loss: 2.21960\n",
      "[1,   336] loss: 2.24174\n",
      "[1,   352] loss: 2.18894\n",
      "[1,   368] loss: 2.19148\n",
      "[1,   384] loss: 2.16115\n",
      "[1,   400] loss: 2.16512\n",
      "[1,   416] loss: 2.16173\n",
      "[1,   432] loss: 2.21420\n",
      "[1,   448] loss: 2.15645\n",
      "[1,   464] loss: 2.15716\n",
      "[1,   480] loss: 2.18131\n",
      "[1,   496] loss: 2.12904\n",
      "[1,   512] loss: 2.10541\n",
      "[1,   528] loss: 2.17108\n",
      "[1,   544] loss: 2.16484\n",
      "[1,   560] loss: 2.16627\n",
      "[1,   576] loss: 2.14330\n",
      "[1,   592] loss: 2.12008\n",
      "[1,   608] loss: 2.09750\n",
      "[1,   624] loss: 2.09665\n",
      "[1,   640] loss: 2.11739\n",
      "[1,   656] loss: 2.15687\n",
      "[1,   672] loss: 2.08399\n",
      "[1,   688] loss: 2.08502\n",
      "[1,   704] loss: 2.04811\n",
      "[1,   720] loss: 2.11007\n",
      "[1,   736] loss: 2.04154\n",
      "[1,   752] loss: 2.11041\n",
      "[1,   768] loss: 2.05728\n",
      "[1,   784] loss: 2.06842\n",
      "[1,   800] loss: 2.06444\n",
      "[1,   816] loss: 2.09244\n",
      "[1,   832] loss: 2.04529\n",
      "[1,   848] loss: 2.10606\n",
      "[1,   864] loss: 2.03208\n",
      "[1,   880] loss: 2.03373\n",
      "[1,   896] loss: 2.07485\n",
      "[1,   912] loss: 2.05005\n",
      "[1,   928] loss: 2.05824\n",
      "[1,   944] loss: 2.07314\n",
      "[1,   960] loss: 2.06068\n",
      "[1,   976] loss: 2.11041\n",
      "[1,   992] loss: 2.03147\n",
      "[1,  1008] loss: 2.06550\n",
      "[1,  1024] loss: 2.08811\n",
      "[1,  1040] loss: 2.08070\n",
      "[1,  1056] loss: 2.11310\n",
      "[1,  1072] loss: 2.04680\n",
      "[1,  1088] loss: 2.09934\n",
      "[1,  1104] loss: 2.08814\n",
      "[1,  1120] loss: 2.06138\n",
      "[1,  1136] loss: 2.10600\n",
      "[1,  1152] loss: 2.08108\n",
      "[1,  1168] loss: 2.08972\n",
      "[1,  1184] loss: 1.97913\n",
      "[1,  1200] loss: 2.02840\n",
      "[1,  1216] loss: 2.08413\n",
      "[1,  1232] loss: 1.98052\n",
      "[1,  1248] loss: 1.96525\n",
      "[1,  1264] loss: 2.04329\n",
      "[1,  1280] loss: 1.99572\n",
      "[1,  1296] loss: 2.04096\n",
      "[1,  1312] loss: 1.95359\n",
      "[1,  1328] loss: 2.02619\n",
      "[1,  1344] loss: 2.02296\n",
      "[1,  1360] loss: 1.86852\n",
      "[1,  1376] loss: 2.01957\n",
      "[1,  1392] loss: 2.01157\n",
      "[1,  1408] loss: 1.97049\n",
      "[1,  1424] loss: 1.96449\n",
      "[1,  1440] loss: 2.01140\n",
      "[1,  1456] loss: 1.93929\n",
      "[1,  1472] loss: 1.92731\n",
      "[1,  1488] loss: 1.84537\n",
      "[1,  1504] loss: 1.95767\n",
      "[1,  1520] loss: 1.95824\n",
      "[1,  1536] loss: 1.96577\n",
      "[1,  1552] loss: 1.98126\n",
      "[1,  1568] loss: 1.95405\n",
      "[1,  1584] loss: 1.89526\n",
      "[1,  1600] loss: 1.97072\n",
      "[1,  1616] loss: 1.99332\n",
      "[1,  1632] loss: 1.89365\n",
      "[1,  1648] loss: 1.93044\n",
      "[1,  1664] loss: 1.98309\n",
      "[1,  1680] loss: 1.99800\n",
      "[1,  1696] loss: 2.01911\n",
      "[1,  1712] loss: 2.01824\n",
      "[1,  1728] loss: 2.02468\n",
      "[1,  1744] loss: 2.02267\n",
      "[1,  1760] loss: 2.01014\n",
      "[1,  1776] loss: 1.96986\n",
      "[1,  1792] loss: 1.98844\n",
      "[1,  1808] loss: 1.94834\n",
      "[1,  1824] loss: 1.95784\n",
      "[1,  1840] loss: 1.91959\n",
      "[1,  1856] loss: 1.99058\n",
      "[1,  1872] loss: 1.96857\n",
      "[1,  1888] loss: 1.96576\n",
      "[1,  1904] loss: 1.92522\n",
      "[1,  1920] loss: 1.98635\n",
      "[1,  1936] loss: 1.95882\n",
      "[1,  1952] loss: 1.99855\n",
      "[1,  1968] loss: 1.98439\n",
      "[1,  1984] loss: 1.95577\n",
      "[1,  2000] loss: 1.96842\n",
      "[1,  2016] loss: 1.94325\n",
      "[1,  2032] loss: 1.89941\n",
      "[1,  2048] loss: 1.95574\n",
      "[1,  2064] loss: 2.03287\n",
      "[1,  2080] loss: 1.95547\n",
      "[1,  2096] loss: 2.04746\n",
      "[1,  2112] loss: 2.02283\n",
      "[1,  2128] loss: 1.94654\n",
      "[1,  2144] loss: 1.86411\n",
      "[1,  2160] loss: 1.96042\n",
      "[1,  2176] loss: 1.92049\n",
      "[1,  2192] loss: 1.94690\n",
      "[1,  2208] loss: 1.94365\n",
      "[1,  2224] loss: 1.89642\n",
      "[1,  2240] loss: 1.91167\n",
      "[1,  2256] loss: 1.98557\n",
      "[1,  2272] loss: 1.92798\n",
      "[1,  2288] loss: 1.82675\n",
      "[1,  2304] loss: 1.89636\n",
      "[1,  2320] loss: 1.96863\n",
      "[1,  2336] loss: 1.96758\n",
      "[1,  2352] loss: 1.96058\n",
      "[1,  2368] loss: 1.91788\n",
      "[1,  2384] loss: 2.07253\n",
      "[1,  2400] loss: 1.98193\n",
      "[1,  2416] loss: 1.92277\n",
      "[1,  2432] loss: 1.81563\n",
      "[1,  2448] loss: 1.83806\n",
      "[1,  2464] loss: 1.96210\n",
      "[1,  2480] loss: 1.98432\n",
      "[1,  2496] loss: 1.96497\n",
      "[1,  2512] loss: 1.94652\n",
      "[1,  2528] loss: 2.00830\n",
      "[1,  2544] loss: 1.83002\n",
      "[1,  2560] loss: 2.04924\n",
      "[1,  2576] loss: 1.96643\n",
      "[1,  2592] loss: 1.93418\n",
      "[1,  2608] loss: 1.96108\n",
      "[1,  2624] loss: 1.85309\n",
      "[1,  2640] loss: 1.98428\n",
      "[1,  2656] loss: 1.83091\n",
      "[1,  2672] loss: 2.08669\n",
      "[1,  2688] loss: 1.97963\n",
      "[1,  2704] loss: 1.89085\n",
      "[1,  2720] loss: 1.95850\n",
      "[1,  2736] loss: 1.93088\n",
      "[1,  2752] loss: 1.96052\n",
      "[1,  2768] loss: 1.92322\n",
      "[1,  2784] loss: 1.95330\n",
      "[1,  2800] loss: 1.91499\n",
      "[1,  2816] loss: 1.87537\n",
      "[1,  2832] loss: 1.93569\n",
      "[1,  2848] loss: 1.99004\n",
      "[1,  2864] loss: 1.95441\n",
      "[1,  2880] loss: 1.90471\n",
      "[1,  2896] loss: 1.87928\n",
      "[1,  2912] loss: 1.92117\n",
      "[1,  2928] loss: 1.92541\n",
      "[1,  2944] loss: 1.97725\n",
      "[1,  2960] loss: 1.82775\n",
      "[1,  2976] loss: 1.93714\n",
      "[1,  2992] loss: 1.82768\n",
      "[1,  3008] loss: 2.04702\n",
      "[1,  3024] loss: 1.86643\n",
      "[1,  3040] loss: 1.97582\n",
      "[1,  3056] loss: 1.95223\n",
      "[1,  3072] loss: 1.93263\n",
      "[1,  3088] loss: 1.83009\n",
      "[1,  3104] loss: 1.92579\n",
      "[1,  3120] loss: 1.90381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739688e2ad4b48b7a4dbfd17454c8705"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,    16] loss: 1.93534\n",
      "[2,    32] loss: 1.85886\n",
      "[2,    48] loss: 1.86332\n",
      "[2,    64] loss: 1.87091\n",
      "[2,    80] loss: 1.86911\n",
      "[2,    96] loss: 1.88764\n",
      "[2,   112] loss: 1.84138\n",
      "[2,   128] loss: 1.92349\n",
      "[2,   144] loss: 1.95233\n",
      "[2,   160] loss: 1.89323\n",
      "[2,   176] loss: 1.92351\n",
      "[2,   192] loss: 1.82410\n",
      "[2,   208] loss: 1.94680\n",
      "[2,   224] loss: 1.87491\n",
      "[2,   240] loss: 1.82010\n",
      "[2,   256] loss: 1.89515\n",
      "[2,   272] loss: 1.92870\n",
      "[2,   288] loss: 1.90460\n",
      "[2,   304] loss: 1.87405\n",
      "[2,   320] loss: 1.97926\n",
      "[2,   336] loss: 1.93512\n",
      "[2,   352] loss: 1.84637\n",
      "[2,   368] loss: 1.86228\n",
      "[2,   384] loss: 1.82682\n",
      "[2,   400] loss: 1.83731\n",
      "[2,   416] loss: 1.86499\n",
      "[2,   432] loss: 1.92254\n",
      "[2,   448] loss: 1.86139\n",
      "[2,   464] loss: 1.86067\n",
      "[2,   480] loss: 1.92056\n",
      "[2,   496] loss: 1.91361\n",
      "[2,   512] loss: 1.78054\n",
      "[2,   528] loss: 1.88927\n",
      "[2,   544] loss: 1.86760\n",
      "[2,   560] loss: 1.91152\n",
      "[2,   576] loss: 1.92507\n",
      "[2,   592] loss: 1.93294\n",
      "[2,   608] loss: 1.82468\n",
      "[2,   624] loss: 1.80455\n",
      "[2,   640] loss: 1.99004\n",
      "[2,   656] loss: 1.90442\n",
      "[2,   672] loss: 1.89932\n",
      "[2,   688] loss: 1.88714\n",
      "[2,   704] loss: 1.85330\n",
      "[2,   720] loss: 1.87940\n",
      "[2,   736] loss: 1.72306\n",
      "[2,   752] loss: 1.89229\n",
      "[2,   768] loss: 1.81372\n",
      "[2,   784] loss: 1.88534\n",
      "[2,   800] loss: 1.89133\n",
      "[2,   816] loss: 1.81227\n",
      "[2,   832] loss: 1.94432\n",
      "[2,   848] loss: 1.89807\n",
      "[2,   864] loss: 1.89370\n",
      "[2,   880] loss: 1.85334\n",
      "[2,   896] loss: 1.84728\n",
      "[2,   912] loss: 1.86664\n",
      "[2,   928] loss: 1.88797\n",
      "[2,   944] loss: 1.96086\n",
      "[2,   960] loss: 1.89286\n",
      "[2,   976] loss: 1.97076\n",
      "[2,   992] loss: 1.86581\n",
      "[2,  1008] loss: 1.89746\n",
      "[2,  1024] loss: 1.94734\n",
      "[2,  1040] loss: 1.88994\n",
      "[2,  1056] loss: 1.99238\n",
      "[2,  1072] loss: 1.92256\n",
      "[2,  1088] loss: 1.93878\n",
      "[2,  1104] loss: 1.88782\n",
      "[2,  1120] loss: 1.93786\n",
      "[2,  1136] loss: 1.94689\n",
      "[2,  1152] loss: 1.88436\n",
      "[2,  1168] loss: 1.93050\n",
      "[2,  1184] loss: 1.83229\n",
      "[2,  1200] loss: 1.83505\n",
      "[2,  1216] loss: 1.88802\n",
      "[2,  1232] loss: 1.83603\n",
      "[2,  1248] loss: 1.79399\n",
      "[2,  1264] loss: 1.90823\n",
      "[2,  1280] loss: 1.86639\n",
      "[2,  1296] loss: 1.91580\n",
      "[2,  1312] loss: 1.80079\n",
      "[2,  1328] loss: 1.85568\n",
      "[2,  1344] loss: 1.88312\n",
      "[2,  1360] loss: 1.71255\n",
      "[2,  1376] loss: 1.89461\n",
      "[2,  1392] loss: 1.87758\n",
      "[2,  1408] loss: 1.84778\n",
      "[2,  1424] loss: 1.82351\n",
      "[2,  1440] loss: 1.92338\n",
      "[2,  1456] loss: 1.83238\n",
      "[2,  1472] loss: 1.75643\n",
      "[2,  1488] loss: 1.73113\n",
      "[2,  1504] loss: 1.78238\n",
      "[2,  1520] loss: 1.75596\n",
      "[2,  1536] loss: 1.80322\n",
      "[2,  1552] loss: 1.85972\n",
      "[2,  1568] loss: 1.80566\n",
      "[2,  1584] loss: 1.84384\n",
      "[2,  1600] loss: 1.85891\n",
      "[2,  1616] loss: 1.88055\n",
      "[2,  1632] loss: 1.73827\n",
      "[2,  1648] loss: 1.75810\n",
      "[2,  1664] loss: 1.84812\n",
      "[2,  1680] loss: 1.82182\n",
      "[2,  1696] loss: 1.87577\n",
      "[2,  1712] loss: 1.87642\n",
      "[2,  1728] loss: 1.87010\n",
      "[2,  1744] loss: 1.90929\n",
      "[2,  1760] loss: 1.91033\n",
      "[2,  1776] loss: 1.84896\n",
      "[2,  1792] loss: 1.82803\n",
      "[2,  1808] loss: 1.73561\n",
      "[2,  1824] loss: 1.85786\n",
      "[2,  1840] loss: 1.82359\n",
      "[2,  1856] loss: 1.85870\n",
      "[2,  1872] loss: 1.86854\n",
      "[2,  1888] loss: 1.84270\n",
      "[2,  1904] loss: 1.80427\n",
      "[2,  1920] loss: 1.87463\n",
      "[2,  1936] loss: 1.82477\n",
      "[2,  1952] loss: 1.85264\n",
      "[2,  1968] loss: 1.86867\n",
      "[2,  1984] loss: 1.87785\n",
      "[2,  2000] loss: 1.91308\n",
      "[2,  2016] loss: 1.87239\n",
      "[2,  2032] loss: 1.81667\n",
      "[2,  2048] loss: 1.87905\n",
      "[2,  2064] loss: 1.91800\n",
      "[2,  2080] loss: 1.84322\n",
      "[2,  2096] loss: 1.97136\n",
      "[2,  2112] loss: 1.91814\n",
      "[2,  2128] loss: 1.86379\n",
      "[2,  2144] loss: 1.73815\n",
      "[2,  2160] loss: 1.82311\n",
      "[2,  2176] loss: 1.85364\n",
      "[2,  2192] loss: 1.83313\n",
      "[2,  2208] loss: 1.85015\n",
      "[2,  2224] loss: 1.79895\n",
      "[2,  2240] loss: 1.85304\n",
      "[2,  2256] loss: 1.87095\n",
      "[2,  2272] loss: 1.82756\n",
      "[2,  2288] loss: 1.74312\n",
      "[2,  2304] loss: 1.81008\n",
      "[2,  2320] loss: 1.89970\n",
      "[2,  2336] loss: 1.86672\n",
      "[2,  2352] loss: 1.88584\n",
      "[2,  2368] loss: 1.82965\n",
      "[2,  2384] loss: 2.01456\n",
      "[2,  2400] loss: 1.92398\n",
      "[2,  2416] loss: 1.81641\n",
      "[2,  2432] loss: 1.74010\n",
      "[2,  2448] loss: 1.71316\n",
      "[2,  2464] loss: 1.86624\n",
      "[2,  2480] loss: 1.93105\n",
      "[2,  2496] loss: 1.88930\n",
      "[2,  2512] loss: 1.82255\n",
      "[2,  2528] loss: 1.90143\n",
      "[2,  2544] loss: 1.71140\n",
      "[2,  2560] loss: 1.96127\n",
      "[2,  2576] loss: 1.86755\n",
      "[2,  2592] loss: 1.85889\n",
      "[2,  2608] loss: 1.87749\n",
      "[2,  2624] loss: 1.83298\n",
      "[2,  2640] loss: 1.90351\n",
      "[2,  2656] loss: 1.73147\n",
      "[2,  2672] loss: 1.97643\n",
      "[2,  2688] loss: 1.90117\n",
      "[2,  2704] loss: 1.79776\n",
      "[2,  2720] loss: 1.87825\n",
      "[2,  2736] loss: 1.84329\n",
      "[2,  2752] loss: 1.82996\n",
      "[2,  2768] loss: 1.84881\n",
      "[2,  2784] loss: 1.86397\n",
      "[2,  2800] loss: 1.86181\n",
      "[2,  2816] loss: 1.76016\n",
      "[2,  2832] loss: 1.87007\n",
      "[2,  2848] loss: 1.85858\n",
      "[2,  2864] loss: 1.87230\n",
      "[2,  2880] loss: 1.83120\n",
      "[2,  2896] loss: 1.78637\n",
      "[2,  2912] loss: 1.85897\n",
      "[2,  2928] loss: 1.83978\n",
      "[2,  2944] loss: 1.85338\n",
      "[2,  2960] loss: 1.75919\n",
      "[2,  2976] loss: 1.87271\n",
      "[2,  2992] loss: 1.76124\n",
      "[2,  3008] loss: 1.99742\n",
      "[2,  3024] loss: 1.77411\n",
      "[2,  3040] loss: 1.90420\n",
      "[2,  3056] loss: 1.88783\n",
      "[2,  3072] loss: 1.84981\n",
      "[2,  3088] loss: 1.71471\n",
      "[2,  3104] loss: 1.82019\n",
      "[2,  3120] loss: 1.78744\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a6d43056144a16baa26d2b5ff0f322"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,    16] loss: 1.81872\n",
      "[3,    32] loss: 1.75473\n",
      "[3,    48] loss: 1.79427\n",
      "[3,    64] loss: 1.80119\n",
      "[3,    80] loss: 1.75679\n",
      "[3,    96] loss: 1.79590\n",
      "[3,   112] loss: 1.76667\n",
      "[3,   128] loss: 1.84920\n",
      "[3,   144] loss: 1.83579\n",
      "[3,   160] loss: 1.83602\n",
      "[3,   176] loss: 1.83347\n",
      "[3,   192] loss: 1.81798\n",
      "[3,   208] loss: 1.85109\n",
      "[3,   224] loss: 1.80931\n",
      "[3,   240] loss: 1.73310\n",
      "[3,   256] loss: 1.86338\n",
      "[3,   272] loss: 1.89409\n",
      "[3,   288] loss: 1.85659\n",
      "[3,   304] loss: 1.78586\n",
      "[3,   320] loss: 1.93813\n",
      "[3,   336] loss: 1.85783\n",
      "[3,   352] loss: 1.79159\n",
      "[3,   368] loss: 1.79396\n",
      "[3,   384] loss: 1.75904\n",
      "[3,   400] loss: 1.74917\n",
      "[3,   416] loss: 1.76314\n",
      "[3,   432] loss: 1.84636\n",
      "[3,   448] loss: 1.73926\n",
      "[3,   464] loss: 1.72827\n",
      "[3,   480] loss: 1.94809\n",
      "[3,   496] loss: 1.79520\n",
      "[3,   512] loss: 1.69953\n",
      "[3,   528] loss: 1.80997\n",
      "[3,   544] loss: 1.75948\n",
      "[3,   560] loss: 1.89998\n",
      "[3,   576] loss: 1.88025\n",
      "[3,   592] loss: 1.85681\n",
      "[3,   608] loss: 1.74311\n",
      "[3,   624] loss: 1.72521\n",
      "[3,   640] loss: 1.93519\n",
      "[3,   656] loss: 1.86515\n",
      "[3,   672] loss: 1.81869\n",
      "[3,   688] loss: 1.80153\n",
      "[3,   704] loss: 1.79842\n",
      "[3,   720] loss: 1.82709\n",
      "[3,   736] loss: 1.63909\n",
      "[3,   752] loss: 1.81881\n",
      "[3,   768] loss: 1.76425\n",
      "[3,   784] loss: 1.82303\n",
      "[3,   800] loss: 1.82564\n",
      "[3,   816] loss: 1.75867\n",
      "[3,   832] loss: 1.87611\n",
      "[3,   848] loss: 1.83913\n",
      "[3,   864] loss: 1.79841\n",
      "[3,   880] loss: 1.75392\n",
      "[3,   896] loss: 1.77280\n",
      "[3,   912] loss: 1.81872\n",
      "[3,   928] loss: 1.80165\n",
      "[3,   944] loss: 1.92639\n",
      "[3,   960] loss: 1.87261\n",
      "[3,   976] loss: 1.94002\n",
      "[3,   992] loss: 1.79343\n",
      "[3,  1008] loss: 1.87047\n",
      "[3,  1024] loss: 1.92586\n",
      "[3,  1040] loss: 1.88404\n",
      "[3,  1056] loss: 1.86182\n",
      "[3,  1072] loss: 1.82123\n",
      "[3,  1088] loss: 1.91370\n",
      "[3,  1104] loss: 1.84904\n",
      "[3,  1120] loss: 1.90666\n",
      "[3,  1136] loss: 1.83764\n",
      "[3,  1152] loss: 1.83126\n",
      "[3,  1168] loss: 1.87628\n",
      "[3,  1184] loss: 1.79456\n",
      "[3,  1200] loss: 1.78356\n",
      "[3,  1216] loss: 1.83981\n",
      "[3,  1232] loss: 1.78303\n",
      "[3,  1248] loss: 1.73868\n",
      "[3,  1264] loss: 1.84272\n",
      "[3,  1280] loss: 1.83803\n",
      "[3,  1296] loss: 1.91062\n",
      "[3,  1312] loss: 1.72213\n",
      "[3,  1328] loss: 1.77299\n",
      "[3,  1344] loss: 1.85256\n",
      "[3,  1360] loss: 1.67938\n",
      "[3,  1376] loss: 1.85625\n",
      "[3,  1392] loss: 1.81232\n",
      "[3,  1408] loss: 1.80314\n",
      "[3,  1424] loss: 1.75586\n",
      "[3,  1440] loss: 1.89497\n",
      "[3,  1456] loss: 1.85130\n",
      "[3,  1472] loss: 1.70906\n",
      "[3,  1488] loss: 1.69425\n",
      "[3,  1504] loss: 1.71657\n",
      "[3,  1520] loss: 1.67498\n",
      "[3,  1536] loss: 1.73363\n",
      "[3,  1552] loss: 1.82380\n",
      "[3,  1568] loss: 1.75088\n",
      "[3,  1584] loss: 1.79048\n",
      "[3,  1600] loss: 1.76721\n",
      "[3,  1616] loss: 1.84959\n",
      "[3,  1632] loss: 1.69616\n",
      "[3,  1648] loss: 1.70342\n",
      "[3,  1664] loss: 1.79765\n",
      "[3,  1680] loss: 1.76820\n",
      "[3,  1696] loss: 1.78400\n",
      "[3,  1712] loss: 1.80728\n",
      "[3,  1728] loss: 1.80455\n",
      "[3,  1744] loss: 1.85207\n",
      "[3,  1760] loss: 1.89586\n",
      "[3,  1776] loss: 1.80636\n",
      "[3,  1792] loss: 1.80300\n",
      "[3,  1808] loss: 1.71372\n",
      "[3,  1824] loss: 1.81670\n",
      "[3,  1840] loss: 1.74935\n",
      "[3,  1856] loss: 1.78242\n",
      "[3,  1872] loss: 1.80721\n",
      "[3,  1888] loss: 1.77861\n",
      "[3,  1904] loss: 1.72512\n",
      "[3,  1920] loss: 1.82936\n",
      "[3,  1936] loss: 1.77911\n",
      "[3,  1952] loss: 1.80776\n",
      "[3,  1968] loss: 1.82161\n",
      "[3,  1984] loss: 1.80359\n",
      "[3,  2000] loss: 1.83267\n",
      "[3,  2016] loss: 1.79114\n",
      "[3,  2032] loss: 1.74368\n",
      "[3,  2048] loss: 1.83273\n",
      "[3,  2064] loss: 1.84491\n",
      "[3,  2080] loss: 1.75108\n",
      "[3,  2096] loss: 1.89395\n",
      "[3,  2112] loss: 1.93037\n",
      "[3,  2128] loss: 1.80668\n",
      "[3,  2144] loss: 1.64278\n",
      "[3,  2160] loss: 1.74054\n",
      "[3,  2176] loss: 1.80560\n",
      "[3,  2192] loss: 1.77879\n",
      "[3,  2208] loss: 1.76427\n",
      "[3,  2224] loss: 1.69448\n",
      "[3,  2240] loss: 1.82683\n",
      "[3,  2256] loss: 1.83892\n",
      "[3,  2272] loss: 1.81415\n",
      "[3,  2288] loss: 1.70272\n",
      "[3,  2304] loss: 1.74189\n",
      "[3,  2320] loss: 1.84419\n",
      "[3,  2336] loss: 1.81362\n",
      "[3,  2352] loss: 1.81761\n",
      "[3,  2368] loss: 1.75143\n",
      "[3,  2384] loss: 1.93262\n",
      "[3,  2400] loss: 1.85328\n",
      "[3,  2416] loss: 1.72285\n",
      "[3,  2432] loss: 1.66545\n",
      "[3,  2448] loss: 1.65841\n",
      "[3,  2464] loss: 1.81082\n",
      "[3,  2480] loss: 1.85884\n",
      "[3,  2496] loss: 1.87361\n",
      "[3,  2512] loss: 1.79277\n",
      "[3,  2528] loss: 1.85162\n",
      "[3,  2544] loss: 1.62777\n",
      "[3,  2560] loss: 1.95707\n",
      "[3,  2576] loss: 1.83865\n",
      "[3,  2592] loss: 1.83951\n",
      "[3,  2608] loss: 1.83605\n",
      "[3,  2624] loss: 1.81147\n",
      "[3,  2640] loss: 1.89306\n",
      "[3,  2656] loss: 1.69814\n",
      "[3,  2672] loss: 1.94381\n",
      "[3,  2688] loss: 1.86372\n",
      "[3,  2704] loss: 1.74393\n",
      "[3,  2720] loss: 1.81630\n",
      "[3,  2736] loss: 1.75478\n",
      "[3,  2752] loss: 1.79336\n",
      "[3,  2768] loss: 1.78344\n",
      "[3,  2784] loss: 1.78172\n",
      "[3,  2800] loss: 1.84663\n",
      "[3,  2816] loss: 1.72820\n",
      "[3,  2832] loss: 1.83553\n",
      "[3,  2848] loss: 1.76685\n",
      "[3,  2864] loss: 1.81720\n",
      "[3,  2880] loss: 1.76276\n",
      "[3,  2896] loss: 1.71273\n",
      "[3,  2912] loss: 1.82021\n",
      "[3,  2928] loss: 1.77787\n",
      "[3,  2944] loss: 1.78007\n",
      "[3,  2960] loss: 1.69770\n",
      "[3,  2976] loss: 1.86240\n",
      "[3,  2992] loss: 1.73090\n",
      "[3,  3008] loss: 1.91399\n",
      "[3,  3024] loss: 1.72038\n",
      "[3,  3040] loss: 1.83838\n",
      "[3,  3056] loss: 1.85145\n",
      "[3,  3072] loss: 1.81781\n",
      "[3,  3088] loss: 1.68256\n",
      "[3,  3104] loss: 1.77357\n",
      "[3,  3120] loss: 1.74634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddb3241d8a64a7ab179b6b47d7f1546"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,    16] loss: 1.72712\n",
      "[4,    32] loss: 1.72438\n",
      "[4,    48] loss: 1.74880\n",
      "[4,    64] loss: 1.73224\n",
      "[4,    80] loss: 1.68013\n",
      "[4,    96] loss: 1.75433\n",
      "[4,   112] loss: 1.70461\n",
      "[4,   128] loss: 1.79459\n",
      "[4,   144] loss: 1.78349\n",
      "[4,   160] loss: 1.79614\n",
      "[4,   176] loss: 1.77895\n",
      "[4,   192] loss: 1.73570\n",
      "[4,   208] loss: 1.81890\n",
      "[4,   224] loss: 1.77043\n",
      "[4,   240] loss: 1.67945\n",
      "[4,   256] loss: 1.81528\n",
      "[4,   272] loss: 1.83514\n",
      "[4,   288] loss: 1.79282\n",
      "[4,   304] loss: 1.77410\n",
      "[4,   320] loss: 1.87671\n",
      "[4,   336] loss: 1.80741\n",
      "[4,   352] loss: 1.75035\n",
      "[4,   368] loss: 1.74021\n",
      "[4,   384] loss: 1.71537\n",
      "[4,   400] loss: 1.67043\n",
      "[4,   416] loss: 1.70378\n",
      "[4,   432] loss: 1.84821\n",
      "[4,   448] loss: 1.65212\n",
      "[4,   464] loss: 1.69315\n",
      "[4,   480] loss: 1.93076\n",
      "[4,   496] loss: 1.76826\n",
      "[4,   512] loss: 1.66953\n",
      "[4,   528] loss: 1.79387\n",
      "[4,   544] loss: 1.69817\n",
      "[4,   560] loss: 1.87336\n",
      "[4,   576] loss: 1.81562\n",
      "[4,   592] loss: 1.78078\n",
      "[4,   608] loss: 1.68618\n",
      "[4,   624] loss: 1.66644\n",
      "[4,   640] loss: 1.86165\n",
      "[4,   656] loss: 1.86758\n",
      "[4,   672] loss: 1.73796\n",
      "[4,   688] loss: 1.75615\n",
      "[4,   704] loss: 1.74493\n",
      "[4,   720] loss: 1.76893\n",
      "[4,   736] loss: 1.57797\n",
      "[4,   752] loss: 1.77642\n",
      "[4,   768] loss: 1.69386\n",
      "[4,   784] loss: 1.76502\n",
      "[4,   800] loss: 1.78068\n",
      "[4,   816] loss: 1.72494\n",
      "[4,   832] loss: 1.82057\n",
      "[4,   848] loss: 1.78844\n",
      "[4,   864] loss: 1.75019\n",
      "[4,   880] loss: 1.68748\n",
      "[4,   896] loss: 1.74350\n",
      "[4,   912] loss: 1.76020\n",
      "[4,   928] loss: 1.73736\n",
      "[4,   944] loss: 1.87846\n",
      "[4,   960] loss: 1.75337\n",
      "[4,   976] loss: 1.84086\n",
      "[4,   992] loss: 1.69721\n",
      "[4,  1008] loss: 1.83219\n",
      "[4,  1024] loss: 1.82640\n",
      "[4,  1040] loss: 1.78313\n",
      "[4,  1056] loss: 1.78772\n",
      "[4,  1072] loss: 1.73180\n",
      "[4,  1088] loss: 1.83962\n",
      "[4,  1104] loss: 1.81670\n",
      "[4,  1120] loss: 1.87845\n",
      "[4,  1136] loss: 1.78099\n",
      "[4,  1152] loss: 1.79848\n",
      "[4,  1168] loss: 1.84915\n",
      "[4,  1184] loss: 1.74161\n",
      "[4,  1200] loss: 1.78079\n",
      "[4,  1216] loss: 1.80392\n",
      "[4,  1232] loss: 1.76541\n",
      "[4,  1248] loss: 1.67129\n",
      "[4,  1264] loss: 1.77201\n",
      "[4,  1280] loss: 1.78120\n",
      "[4,  1296] loss: 1.87947\n",
      "[4,  1312] loss: 1.67704\n",
      "[4,  1328] loss: 1.70831\n",
      "[4,  1344] loss: 1.83860\n",
      "[4,  1360] loss: 1.63246\n",
      "[4,  1376] loss: 1.81764\n",
      "[4,  1392] loss: 1.78650\n",
      "[4,  1408] loss: 1.73318\n",
      "[4,  1424] loss: 1.71243\n",
      "[4,  1440] loss: 1.83404\n",
      "[4,  1456] loss: 1.77992\n",
      "[4,  1472] loss: 1.64693\n",
      "[4,  1488] loss: 1.63576\n",
      "[4,  1504] loss: 1.72041\n",
      "[4,  1520] loss: 1.64754\n",
      "[4,  1536] loss: 1.70138\n",
      "[4,  1552] loss: 1.76748\n",
      "[4,  1568] loss: 1.69576\n",
      "[4,  1584] loss: 1.76386\n",
      "[4,  1600] loss: 1.72764\n",
      "[4,  1616] loss: 1.83395\n",
      "[4,  1632] loss: 1.67859\n",
      "[4,  1648] loss: 1.67712\n",
      "[4,  1664] loss: 1.75414\n",
      "[4,  1680] loss: 1.71275\n",
      "[4,  1696] loss: 1.74153\n",
      "[4,  1712] loss: 1.73918\n",
      "[4,  1728] loss: 1.80097\n",
      "[4,  1744] loss: 1.81744\n",
      "[4,  1760] loss: 1.81745\n",
      "[4,  1776] loss: 1.76265\n",
      "[4,  1792] loss: 1.77302\n",
      "[4,  1808] loss: 1.67679\n",
      "[4,  1824] loss: 1.75940\n",
      "[4,  1840] loss: 1.71234\n",
      "[4,  1856] loss: 1.75659\n",
      "[4,  1872] loss: 1.74508\n",
      "[4,  1888] loss: 1.71696\n",
      "[4,  1904] loss: 1.67122\n",
      "[4,  1920] loss: 1.81381\n",
      "[4,  1936] loss: 1.75446\n",
      "[4,  1952] loss: 1.78313\n",
      "[4,  1968] loss: 1.78922\n",
      "[4,  1984] loss: 1.76127\n",
      "[4,  2000] loss: 1.78284\n",
      "[4,  2016] loss: 1.75867\n",
      "[4,  2032] loss: 1.71997\n",
      "[4,  2048] loss: 1.75644\n",
      "[4,  2064] loss: 1.82128\n",
      "[4,  2080] loss: 1.70480\n",
      "[4,  2096] loss: 1.82699\n",
      "[4,  2112] loss: 1.91134\n",
      "[4,  2128] loss: 1.80669\n",
      "[4,  2144] loss: 1.60269\n",
      "[4,  2160] loss: 1.70547\n",
      "[4,  2176] loss: 1.77669\n",
      "[4,  2192] loss: 1.76340\n",
      "[4,  2208] loss: 1.73144\n",
      "[4,  2224] loss: 1.66440\n",
      "[4,  2240] loss: 1.78882\n",
      "[4,  2256] loss: 1.81512\n",
      "[4,  2272] loss: 1.77071\n",
      "[4,  2288] loss: 1.64746\n",
      "[4,  2304] loss: 1.71011\n",
      "[4,  2320] loss: 1.82008\n",
      "[4,  2336] loss: 1.75737\n",
      "[4,  2352] loss: 1.78282\n",
      "[4,  2368] loss: 1.70019\n",
      "[4,  2384] loss: 1.85692\n",
      "[4,  2400] loss: 1.82498\n",
      "[4,  2416] loss: 1.68718\n",
      "[4,  2432] loss: 1.63103\n",
      "[4,  2448] loss: 1.61103\n",
      "[4,  2464] loss: 1.76050\n",
      "[4,  2480] loss: 1.83059\n",
      "[4,  2496] loss: 1.83066\n",
      "[4,  2512] loss: 1.79058\n",
      "[4,  2528] loss: 1.82910\n",
      "[4,  2544] loss: 1.59930\n",
      "[4,  2560] loss: 1.91404\n",
      "[4,  2576] loss: 1.80510\n",
      "[4,  2592] loss: 1.82171\n",
      "[4,  2608] loss: 1.83524\n",
      "[4,  2624] loss: 1.78556\n",
      "[4,  2640] loss: 1.86556\n",
      "[4,  2656] loss: 1.64976\n",
      "[4,  2672] loss: 1.89740\n",
      "[4,  2688] loss: 1.79422\n",
      "[4,  2704] loss: 1.73676\n",
      "[4,  2720] loss: 1.78351\n",
      "[4,  2736] loss: 1.72321\n",
      "[4,  2752] loss: 1.75500\n",
      "[4,  2768] loss: 1.74487\n",
      "[4,  2784] loss: 1.76764\n",
      "[4,  2800] loss: 1.82492\n",
      "[4,  2816] loss: 1.68822\n",
      "[4,  2832] loss: 1.76222\n",
      "[4,  2848] loss: 1.70084\n",
      "[4,  2864] loss: 1.78960\n",
      "[4,  2880] loss: 1.72324\n",
      "[4,  2896] loss: 1.68752\n",
      "[4,  2912] loss: 1.78813\n",
      "[4,  2928] loss: 1.73775\n",
      "[4,  2944] loss: 1.74599\n",
      "[4,  2960] loss: 1.65712\n",
      "[4,  2976] loss: 1.82699\n",
      "[4,  2992] loss: 1.68097\n",
      "[4,  3008] loss: 1.88922\n",
      "[4,  3024] loss: 1.66282\n",
      "[4,  3040] loss: 1.82829\n",
      "[4,  3056] loss: 1.82311\n",
      "[4,  3072] loss: 1.79008\n",
      "[4,  3088] loss: 1.67882\n",
      "[4,  3104] loss: 1.73120\n",
      "[4,  3120] loss: 1.68270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82439263b1a4c7eb22cd39c48181c6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,    16] loss: 1.69392\n",
      "[5,    32] loss: 1.72918\n",
      "[5,    48] loss: 1.72154\n",
      "[5,    64] loss: 1.72194\n",
      "[5,    80] loss: 1.65962\n",
      "[5,    96] loss: 1.72155\n",
      "[5,   112] loss: 1.68612\n",
      "[5,   128] loss: 1.77817\n",
      "[5,   144] loss: 1.72563\n",
      "[5,   160] loss: 1.74230\n",
      "[5,   176] loss: 1.73996\n",
      "[5,   192] loss: 1.70702\n",
      "[5,   208] loss: 1.76976\n",
      "[5,   224] loss: 1.71451\n",
      "[5,   240] loss: 1.65204\n",
      "[5,   256] loss: 1.79736\n",
      "[5,   272] loss: 1.80908\n",
      "[5,   288] loss: 1.76801\n",
      "[5,   304] loss: 1.76430\n",
      "[5,   320] loss: 1.88864\n",
      "[5,   336] loss: 1.76761\n",
      "[5,   352] loss: 1.69206\n",
      "[5,   368] loss: 1.72591\n",
      "[5,   384] loss: 1.65838\n",
      "[5,   400] loss: 1.62263\n",
      "[5,   416] loss: 1.67934\n",
      "[5,   432] loss: 1.84346\n",
      "[5,   448] loss: 1.64084\n",
      "[5,   464] loss: 1.66942\n",
      "[5,   480] loss: 1.88678\n",
      "[5,   496] loss: 1.74415\n",
      "[5,   512] loss: 1.65897\n",
      "[5,   528] loss: 1.78092\n",
      "[5,   544] loss: 1.64718\n",
      "[5,   560] loss: 1.85018\n",
      "[5,   576] loss: 1.78635\n",
      "[5,   592] loss: 1.72789\n",
      "[5,   608] loss: 1.64510\n",
      "[5,   624] loss: 1.62813\n",
      "[5,   640] loss: 1.82704\n",
      "[5,   656] loss: 1.82143\n",
      "[5,   672] loss: 1.68007\n",
      "[5,   688] loss: 1.70700\n",
      "[5,   704] loss: 1.69518\n",
      "[5,   720] loss: 1.73407\n",
      "[5,   736] loss: 1.55324\n",
      "[5,   752] loss: 1.74809\n",
      "[5,   768] loss: 1.64975\n",
      "[5,   784] loss: 1.71967\n",
      "[5,   800] loss: 1.77889\n",
      "[5,   816] loss: 1.70658\n",
      "[5,   832] loss: 1.77207\n",
      "[5,   848] loss: 1.71709\n",
      "[5,   864] loss: 1.70321\n",
      "[5,   880] loss: 1.66426\n",
      "[5,   896] loss: 1.73257\n",
      "[5,   912] loss: 1.72623\n",
      "[5,   928] loss: 1.69377\n",
      "[5,   944] loss: 1.84457\n",
      "[5,   960] loss: 1.69840\n",
      "[5,   976] loss: 1.81866\n",
      "[5,   992] loss: 1.65396\n",
      "[5,  1008] loss: 1.81704\n",
      "[5,  1024] loss: 1.78369\n",
      "[5,  1040] loss: 1.76175\n",
      "[5,  1056] loss: 1.76312\n",
      "[5,  1072] loss: 1.68861\n",
      "[5,  1088] loss: 1.81850\n",
      "[5,  1104] loss: 1.79320\n",
      "[5,  1120] loss: 1.86141\n",
      "[5,  1136] loss: 1.76614\n",
      "[5,  1152] loss: 1.74947\n",
      "[5,  1168] loss: 1.78565\n",
      "[5,  1184] loss: 1.74088\n",
      "[5,  1200] loss: 1.74178\n",
      "[5,  1216] loss: 1.81437\n",
      "[5,  1232] loss: 1.76391\n",
      "[5,  1248] loss: 1.66035\n",
      "[5,  1264] loss: 1.73430\n",
      "[5,  1280] loss: 1.77366\n",
      "[5,  1296] loss: 1.86568\n",
      "[5,  1312] loss: 1.65644\n",
      "[5,  1328] loss: 1.68621\n",
      "[5,  1344] loss: 1.77934\n",
      "[5,  1360] loss: 1.59341\n",
      "[5,  1376] loss: 1.77320\n",
      "[5,  1392] loss: 1.75336\n",
      "[5,  1408] loss: 1.70735\n",
      "[5,  1424] loss: 1.66702\n",
      "[5,  1440] loss: 1.80166\n",
      "[5,  1456] loss: 1.75126\n",
      "[5,  1472] loss: 1.63231\n",
      "[5,  1488] loss: 1.58430\n",
      "[5,  1504] loss: 1.68500\n",
      "[5,  1520] loss: 1.61655\n",
      "[5,  1536] loss: 1.65960\n",
      "[5,  1552] loss: 1.73726\n",
      "[5,  1568] loss: 1.65706\n",
      "[5,  1584] loss: 1.75485\n",
      "[5,  1600] loss: 1.69190\n",
      "[5,  1616] loss: 1.81087\n",
      "[5,  1632] loss: 1.61444\n",
      "[5,  1648] loss: 1.67541\n",
      "[5,  1664] loss: 1.70902\n",
      "[5,  1680] loss: 1.70437\n",
      "[5,  1696] loss: 1.69879\n",
      "[5,  1712] loss: 1.72358\n",
      "[5,  1728] loss: 1.75730\n",
      "[5,  1744] loss: 1.79900\n",
      "[5,  1760] loss: 1.78114\n",
      "[5,  1776] loss: 1.73134\n",
      "[5,  1792] loss: 1.74599\n",
      "[5,  1808] loss: 1.62475\n",
      "[5,  1824] loss: 1.75026\n",
      "[5,  1840] loss: 1.69246\n",
      "[5,  1856] loss: 1.74465\n",
      "[5,  1872] loss: 1.72533\n",
      "[5,  1888] loss: 1.66352\n",
      "[5,  1904] loss: 1.65529\n",
      "[5,  1920] loss: 1.78438\n",
      "[5,  1936] loss: 1.72857\n",
      "[5,  1952] loss: 1.77753\n",
      "[5,  1968] loss: 1.78753\n",
      "[5,  1984] loss: 1.74258\n",
      "[5,  2000] loss: 1.75637\n",
      "[5,  2016] loss: 1.71568\n",
      "[5,  2032] loss: 1.68367\n",
      "[5,  2048] loss: 1.71813\n",
      "[5,  2064] loss: 1.77350\n",
      "[5,  2080] loss: 1.67492\n",
      "[5,  2096] loss: 1.81468\n",
      "[5,  2112] loss: 1.87647\n",
      "[5,  2128] loss: 1.76031\n",
      "[5,  2144] loss: 1.55504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-23:\n",
      "Process Process-24:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Applications/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/Applications/anaconda/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,  2160] loss: 1.69833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x1c4904b9e8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Applications/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Applications/anaconda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 167, in rebuild_storage_filename\n",
      "    storage = cls._new_shared_filename(manager, handle, size)\n",
      "  File \"/Applications/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 7290) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e17afb0ef4b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_net_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Adarsh/Documents/Adarsh's Files/3. JUNIOR YEAR/CS294-131/robust_embeddings/models/base_network.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_loader, epochs, opt, criterion, save)\u001b[0m\n\u001b[1;32m     70\u001b[0m                                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                                 \u001b[0mrun_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emb_net.train_model(train_loader, 15, emb_net_opt, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_net.save_model(\"saved_models/emb_net_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model once\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36.7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_net.test_model_once(test_loader, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model once\n",
      "Evaluating model once\n",
      "Evaluating model once\n",
      "Evaluating model once\n",
      "Evaluating model once\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[36.61, 36.61, 36.65, 36.45, 36.62]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_net.test_model(test_loader, gaussian_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e58a407b7b4e0fa1004cd8e4ec2a99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    16] loss: 52.72527\n",
      "[1,    32] loss: 2.60997\n",
      "[1,    48] loss: 2.30600\n",
      "[1,    64] loss: 2.31226\n",
      "[1,    80] loss: 2.30366\n",
      "[1,    96] loss: 2.30575\n",
      "[1,   112] loss: 2.30334\n",
      "[1,   128] loss: 2.30697\n",
      "[1,   144] loss: 2.30553\n",
      "[1,   160] loss: 2.30706\n",
      "[1,   176] loss: 2.31166\n",
      "[1,   192] loss: 2.30756\n",
      "[1,   208] loss: 2.30496\n",
      "[1,   224] loss: 2.30650\n",
      "[1,   240] loss: 2.30314\n",
      "[1,   256] loss: 2.30307\n",
      "[1,   272] loss: 2.30361\n",
      "[1,   288] loss: 2.30522\n",
      "[1,   304] loss: 2.30741\n",
      "[1,   320] loss: 2.30394\n",
      "[1,   336] loss: 2.30833\n",
      "[1,   352] loss: 2.29948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-46:\n",
      "Process Process-45:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Applications/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/Applications/anaconda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-47d274325dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreg_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_net_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Adarsh/Documents/Adarsh's Files/3. JUNIOR YEAR/CS294-131/robust_embeddings/models/base_network.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_loader, epochs, opt, criterion, save)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reg_net.train_model(train_loader, 15, reg_net_opt, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reg_net.save_model(\"saved_models/reg_net_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_net.load_model(\"saved_models/reg_net_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_net.test_model_once(test_loader, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_net.test_model(test_loader, gaussian_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# emb_net.train_model(train_loader, 15, emb_net_opt, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR10(root='../data_cifar_10', train=True,\n",
    "                                       download=False, transform=transform.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = datasets.CIFAR10(root='../data_cifar_10', train=False,\n",
    "                                       download=False, transform=transform.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
